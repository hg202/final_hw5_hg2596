---
title: "final_hw5_hg2596"
output: github_document
date: "2022-11-16"
---

```{r}
library(tidyverse)
library(lubridate)
library(tidyr)
library(plotly)
library(dplyr)
library(plyr)
```

# Problem 2 

```{r}
homicide_1= read_csv("./data/homicide_data.csv", show_col_types = FALSE)
```

# Creating a City State Variable 

```{r}
homicide_1 = homicide_1 %>%
  unite('city_state', city:state, remove = FALSE) %>% 
  apply(., 2, function(city_state) as.character(gsub("_", ",", city_state))) 

```

The homicide raw data has `r nrow(homicide_1) ` observations and `r ncol(homicide_1)`  variables. 
Key variables are **uid** which gives a unique ID to each homicide victim and the **city**, **state** in which the killing took place. Another important variable is the **deposition** which describes the status of the case for examples is it closed with an arrest or possibly still open with no arrest. 

# Cleaning Data and Creating Status Variable 

```{r}
homicide_2 = as_tibble(homicide_1) %>%
  janitor::clean_names() %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  mutate(lat = as.numeric(lat)) %>%
  mutate(lon = as.numeric(lon)) %>%
  mutate(status = ifelse(disposition%in%c("Closed without arrest","Open/No arrest"), 1, 0))
```

```{r}
sum_hc = homicide_2 %>%
  group_by(city) %>%
  dplyr::summarize(n_obs = n(), 
            n_unsolved = sum(status)) 
```

```{r}
baltimore_hc = sum_hc %>% 
  filter(city == "Baltimore") 
```

# Prop test Baltimore

```{r}
first_prop = prop.test(x = pull(baltimore_hc,n_unsolved), n = pull(baltimore_hc,n_obs)) %>%
  broom::tidy()
```

# Prop test all citities 

# Make a dataset 

```{r}
 hold_xy = 
  tibble(homicide_2 %>%
    group_by(city) %>%
    dplyr::summarize(n_obs = n(), 
            n_unsolved = sum(status)))
```

# Make a Function with Prop.test 

```{r}
function_xy = function(x, y) {

 prop.test(x,y) %>%
    broom::tidy()
}
```

# Map Over Function and Clean

```{r}
map2_holdxy = 
 hold_xy %>%
  mutate(
    estimate_df = 
      map2(.x = n_unsolved,.y = n_obs, ~function_xy(x = .x, y = .y))
  ) %>% 
  unnest(estimate_df) %>%
  janitor::clean_names() %>%
  select( - parameter, -method, -alternative, -statistic) 

```


# Graph Estimates and Cl's 


```{r}
graph_p_1 = 
  map2_holdxy %>%
  mutate(city = fct_reorder(city,estimate)) %>%
  ggplot(aes(x = city, y = estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + 
  labs(
    title = "Scatter Plot of Estimates with Error Bars",
    x = "Cities",
    y = "Estimates",
  ) + 
  theme(axis.text.x=element_text(angle=60,vjust = 1, hjust=1,size=10))

graph_p_1  
```

# Problem 3 

# Create a Function 

```{r}
function_a = function(true_mean) 
{
  x = tibble(rnorm(30,true_mean,5))
  
  t.test(x, mu = 0) %>%
    broom::tidy()

}
```

# Map over Function for Mu = 0

```{r cache=TRUE}
map_df = 
  expand_grid(
    true_mean = 0, 
    iter = 1:5000) %>% 
  mutate(
    estimate_df = map(.x = true_mean,~function_a(true_mean = .x))) %>%
  unnest(estimate_df)
 
```

# Map over Function for Mu = 1:6 

```{r cache=TRUE}
map_df = 
  expand_grid(
    true_mean = 1:6, 
    iter = 1:5000) %>% 
  mutate(
    estimate_df = map(.x = true_mean,~function_a(true_mean = .x))) %>%
  unnest(estimate_df)
 
```

# Clean data

```{r}
final_map = 
  map_df %>%
  janitor::clean_names()%>%
  mutate(estimate = as.numeric(estimate)) %>%
  mutate(p_value = as.numeric(p_value)) %>%
  mutate(true_mean = as.character(true_mean)) %>%
  mutate(reject_null = ifelse(p_value < 0.05, 1, 0)) %>%
  mutate(reject_null = as.numeric(reject_null)) %>%
  mutate(mu_hat = estimate) %>%
  select(true_mean, iter, mu_hat, p_value,reject_null) 
```

# Summarize 1

```{r}
final_map_2 = final_map %>%
  group_by(true_mean) %>%
  dplyr::summarize(n_obs = n(), 
            probability_reject = (sum(reject_null))/n_obs,
            ave_e = mean(mu_hat)) 
```

# Summarize 2

```{r}
final_map_3 = 
  final_map %>%
  filter(reject_null == 1) %>%
  group_by(true_mean) %>%
  dplyr::summarize(n_obs = n(), 
            probability_reject = (sum(reject_null))/n_obs,
            ave_e = mean(mu_hat)) 
```

# Graph 1 

```{r}
graph_1 = 
  final_map_2 %>%
  plot_ly(x = ~true_mean, y = ~probability_reject, type = "scatter", mode = "markers",color = ~ true_mean, alpha = 1.2)%>% 
  layout(title = 'Power vs True Mean Values',
         xaxis = list(title = 'True Mean Values'),
         yaxis = list(title = 'Power of Test (probability of being rejected)'), 
         legend = list(title=list(text='<b> True Mean Values </b>'))) 

graph_1
```

Since effect size is impacted by how far the mean estimates are from the 'true' mean of '0', this graph shows as the true means gets farther away, the **effect size increases** which **increases the power** (the probability of rejecting the null hypothesis). The graph also shows that around true mean value of **5 and 6**, the power of the test doesn't change as much because **once you've reached a certain effect size** it's almost certain that your power for the test will be very high, leading you to reject. 

# Graph 2 

```{r}
graph_2 = 
  final_map_2 %>%
  plot_ly(x = ~true_mean, y = ~ave_e, type = "scatter", mode = "markers",color = ~ true_mean, alpha = 1.2) %>% 
  layout(title = 'Average Mean Estimates vs True Mean Values',
         xaxis = list(title = 'True Mean Values'),
         yaxis = list(title = 'Average Mean Estimates'), 
         legend = list(title=list(text='<b> True Mean Values </b>')))

graph_2
```

# Graph 3 

```{r}
graph_3 = 
  final_map_3 %>%
  plot_ly(x = ~true_mean, y = ~ave_e, type = "scatter", mode = "markers",color = ~ true_mean, alpha = 1.2) %>%
  layout(title = 'Average Mean Estimates vs True Mean Values (when null was rejected)',
         xaxis = list(title = 'True Mean Values'),
         yaxis = list(title = 'Average Mean Estimates'), 
         legend = list(title=list(text='<b> True Mean Values </b>')))

graph_3 
```

For **graph 2** we can notice that when the **average estimate of mean** are plotted across the **true mean**, it seems as though the average mean estimates are able to provide the values for true mean, which could be supported by the fact that we has **large sample sizes** therefore better ability to estimate the true mean. 

However, **graph 3** when the data set is **restricted** to just the samples for which the null was rejected, we notice that the **average mean estimates** are not good estimates of the **true mean values** for **small** true mean values. 

When true mean values are low, sample average of means across (test for samples that reject the null) do not approximately equal the true mean values, however, for **true mean values of 4, 5 and 6** they do approximately equal the true mean values.

This is what we expect to see because the lower true mean(1,2,3) are will have lower probability for rejecting the null, which means they have **lower sample size**, low sample size makes it **harder to approximate** the true mean. However, for higher values of true mean (4,5,6) we expect more values to reject the null, therefore, **higher sample size** which leads to **better approximation** of the true mean value. 




